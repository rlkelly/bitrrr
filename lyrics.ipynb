{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pattern\n",
    "from pattern.web import URL, DOM, plaintext\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random\n",
    "\n",
    "filedic = {}\n",
    "for each in os.listdir('lyrics'):\n",
    "    if each != '.DS_Store':\n",
    "        filedic[each] = listdir('lyrics/'+each)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyricsdic = defaultdict(list)\n",
    "exclude = set(string.punctuation)\n",
    "lyricslist = []\n",
    "\n",
    "remove_list = ['verse','hook','chorus', '','choruslloyd banks', '50 cent', 'outro','bridge', 'chorus', 'lyrics', 'lloyd banks', 'rick ross','lil wayne', 'da', 'dem', 'im', 'ya', 'dat', 'tony yayo', 'banks yay 50',\n",
    "              'verse 1 tony yayo', 'verse 4 lloyd banks', 'verse 5 50 cent', 'verse 2 tony yayo', 'verse 1 50 cent', 'chorus 50 cent', 'intro 50 cent', 'biggie sample  in the background throughout the song', 'verse 3  tony yayo', 'verse 1  50 cent','chorus  50 cent','verse 2  lloyd banks','verse 2 lloyd banks', 'verse 6 tony yayo', 'verse 4 yony yayo']\n",
    "\n",
    "for k,v in filedic.iteritems():\n",
    "    for link in v:\n",
    "        if link != '.ipynb_checkpoints':\n",
    "            page = open('lyrics/'+k+'/'+link).read()\n",
    "            dom = DOM(page)\n",
    "            lyrics = dom('.lyrics')[0]\n",
    "            p = plaintext(lyrics.content)\n",
    "            p = ''.join(ch for ch in p if ch not in exclude)\n",
    "            lyricslist.append(p)\n",
    "            p = p.splitlines()\n",
    "            lyricsdic[k].append(filter(lambda x: x.lower() not in remove_list, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lineslist = []\n",
    "remove_list = ['verse','hook','chorus', '','choruslloyd banks', '50 cent', 'outro','bridge', 'chorus', 'lyrics', 'lloyd banks', 'rick ross','lil wayne', 'da', 'dem', 'im', 'ya', 'dat', 'tony yayo', 'banks yay 50',\n",
    "              'verse 1 tony yayo', 'verse 4 lloyd banks', 'verse 5 50 cent', 'verse 2 tony yayo', 'verse 1 50 cent', 'chorus 50 cent', 'intro 50 cent', 'biggie sample  in the background throughout the song', 'verse 3  tony yayo', 'verse 1  50 cent','chorus  50 cent','verse 2  lloyd banks','verse 2 lloyd banks', 'verse 6 tony yayo', 'verse 4 yony yayo']\n",
    "exportlist = []\n",
    "for each in lyricslist:\n",
    "    lineslist.append(each.splitlines())\n",
    "lineslist = np.array(lineslist)\n",
    "for each in lineslist:\n",
    "    exportlist.append(filter(lambda x: x.lower() not in remove_list, each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# f = open('picklelyrics', 'wb')\n",
    "\n",
    "# pickle.dump(lyricsdic, f)\n",
    "\n",
    "# # import simplejson\n",
    "# # f = open('lyricsarchive.txt', 'w')\n",
    "# # simplejson.dump(exportlist, f)\n",
    "# # f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenlist = []\n",
    "\n",
    "for lyrics in lyricslist[:100]:\n",
    "    tokens = nltk.word_tokenize(lyrics)\n",
    "    tokens = [w for w in tokens if w.lower() not in ['50','cent','chief','keef', 'lloyd', 'banks', 'busta','rhymes']]\n",
    "    tokenlist.extend([w for w in tokens if w.lower() not in stopwords.words('english')])\n",
    "\n",
    "import random\n",
    "\n",
    "class Markov(object):\n",
    "\n",
    "    def __init__(self, words):\n",
    "        self.cache = {}\n",
    "        #self.open_file = open_file\n",
    "        self.words = words\n",
    "        self.word_size = len(self.words)\n",
    "        self.database()\n",
    "\n",
    "    def file_to_words(self):\n",
    "        self.open_file.seek(0)\n",
    "        data = self.open_file.read()\n",
    "        words = data.split()\n",
    "        return words\n",
    "\n",
    "    def triples(self):\n",
    "        if len(self.words) < 3:\n",
    "            return\n",
    "        for i in range(len(self.words) - 2):\n",
    "            yield (self.words[i], self.words[i+1], self.words[i+2])\n",
    "\n",
    "    def database(self):\n",
    "        for w1, w2, w3 in self.triples():\n",
    "            key = (w1, w2)\n",
    "            if key in self.cache:\n",
    "                self.cache[key].append(w3)\n",
    "            else:\n",
    "                self.cache[key] = [w3]\n",
    "\n",
    "    def generate_markov_text(self, size=25):\n",
    "        seed = random.randint(0, self.word_size-3)\n",
    "        seed_word, next_word = self.words[seed], self.words[seed+1]\n",
    "        w1, w2 = seed_word, next_word\n",
    "        gen_words = []\n",
    "        for i in xrange(size):\n",
    "            gen_words.append(w1)\n",
    "            w1, w2 = w2, random.choice(self.cache[(w1, w2)])\n",
    "        gen_words.append(w2)\n",
    "        return ' '.join(gen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Dont start lying tire flat catching allergy attack Ill axe battle bat Saddling hos like yeeehaw Bitch enjoy tour youre fuckin Doc 5 Oclock free ride Ambulance late paper thin Hook Bun B Pass cigar Ill bust open Ill dump Dig bag tricks pull lil somethin hold medical marijuana thats break'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark = Markov(tokenlist)\n",
    "mark.generate_markov_text(size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for each cluster:\n",
      "0: busta, rhymes, yo, yall, flipmode\n",
      "1: 50, cent, banks, lloyd, hook\n",
      "2: based, basedgod, bitches, god, lil\n",
      "3: hook, em, man, money, ya\n",
      "4: wayne, ya, money, lil, baby\n",
      "5: chief, keef, sosa, hook, bang\n",
      "6: lil, man, feel, real, love\n",
      "7: cube, ice, big, yo, ll\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df = .6)\n",
    "X = vectorizer.fit_transform(lyricslist)\n",
    "features = vectorizer.get_feature_names()\n",
    "kmeans = KMeans()\n",
    "kmeans.fit(X)\n",
    "\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-6:-1]\n",
    "print \"top features for each cluster:\"\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rhyme(inp, level):\n",
    "    entries = nltk.corpus.cmudict.entries()\n",
    "    syllables = [(word, syl) for word, syl in entries if word == inp]\n",
    "    rhymes = []\n",
    "    for (word, syllable) in syllables:\n",
    "        rhymes += [word for word, pron in entries if pron[-level:] == syllable[-level:]]\n",
    "    return set(rhymes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like ho in his back bust over the cop everything it \n",
      "well was the tricks i could duke him drawls keep \n",
      "intro their business escaped \n",
      "it aint she lit operation spot \n",
      "and my back my return i see last \n",
      "yodleehheeewho kneed my ju herb stopper whole its fake go \n",
      "doom jaw power spell care so they killed this man cage \n",
      "and i are you buggin guns the dudeâ€™ in effect \n",
      "the speakeasy hes not sister \n",
      "magnum the much back \n",
      "on he more that back trunk need leash \n",
      "open hear song your tapes verse \n",
      "always i wanted square effects him found later \n",
      "and gun tape \n",
      "package of the tin in disease fuel feet \n",
      "im me like a nic flow \n",
      "im this worst notch \n",
      "globe told cats in monkey no home so wide pockets \n",
      "stashes of the nation of in out whole cup rappers \n",
      "or i slithered follow crucifix it robbed the rhymes \n",
      "nor cremate gym what her know yet in jakes on wrong rhymes \n",
      "watson it ghetto this razorthin to get to talk \n",
      "verse a bind lines to get a pen summer \n",
      "his then laugh sun of they \n",
      "your every girlfriend teeth \n",
      "just uh the sidney when we need the cent rave \n",
      "and me meditate me before the shit with acting divine \n",
      "you stunt they were to pretend them poked i her a he used few \n",
      "the lets through these verse aight the somethin aint \n",
      "and you do we mint a egg with the been life \n",
      "well these state yalls again analyzing \n",
      "before an wan brother i knows a thickest throne penance \n",
      "some cover snitches just i so got peeps \n",
      "upper far see crack on go peoples \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenlines = []\n",
    "wordtypes = defaultdict(list)\n",
    "\n",
    "for each in lyricsdic['mfdoom']:\n",
    "    for line in each:\n",
    "        tokens = nltk.word_tokenize(line)\n",
    "        struct = nltk.pos_tag(tokens)\n",
    "        for each in struct:\n",
    "            wordtypes[each[1]].append(each[0])\n",
    "        tokenlines.append(','.join([x[1] for x in struct]))\n",
    "\n",
    "sf = []\n",
    "for each in lyricsdic['mfdoom'][0]:\n",
    "    l = nltk.word_tokenize(each)\n",
    "    t = nltk.pos_tag(l)\n",
    "    sf.extend([x[1] for x in t])\n",
    "    sf.extend('\\n ')\n",
    "sf = ' '.join(sf)\n",
    "for each in sf.splitlines():\n",
    "    sentence = \"\"\n",
    "    for a in each.split():\n",
    "        sentence +=  random.choice(wordtypes[a]).lower() + \" \"\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i got a glass they capped a breakfast \n",
      "\n",
      "my fools and his lame yours\n"
     ]
    }
   ],
   "source": [
    "Counter(tokenlines).most_common(25)\n",
    "sentence = \"\"\n",
    "\n",
    "for each in ['PRP','VBD','DT','NN','PRP','VBD','DT','NN']:\n",
    "    sentence +=  random.choice(wordtypes[each]).lower() + \" \"\n",
    "\n",
    "print sentence\n",
    "# last_word = sentence.rsplit(None, 1)[-1]\n",
    "print \"\"\n",
    "\n",
    "for each in ['PRP$','NNS','CC','PRP$','NN','NNP']:\n",
    "     print random.choice(wordtypes[each]).lower(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postag = []\n",
    "for line in tokenlines[0:10]:\n",
    "    tags = nltk.pos_tag(line)\n",
    "    #print tags\n",
    "    #postag.append([[x[1] for x in tags]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "somethin the hostility it have you believe crazy thats \n",
      "\n",
      "they come when to be gingerbread\n"
     ]
    }
   ],
   "source": [
    "Counter(tokenlines).most_common(10)\n",
    "sentence = \"\"\n",
    "\n",
    "for each in ['IN','DT','NN','PRP','VBP','PRP','VBP','JJ','NNS']:\n",
    "    sentence +=  random.choice(wordtypes[each]).lower() + \" \"\n",
    "\n",
    "print sentence\n",
    "last_word = sentence.rsplit(None, 1)[-1]\n",
    "print \"\"\n",
    "\n",
    "for each in ['PRP','VBP','WRB','TO','VB','NN']:\n",
    "    print random.choice(wordtypes[each]).lower(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
