{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pattern\n",
    "from pattern.web import URL, DOM, plaintext\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random\n",
    "\n",
    "filedic = {}\n",
    "for each in os.listdir('lyrics'):\n",
    "    if each != '.DS_Store':\n",
    "        filedic[each] = listdir('lyrics/'+each)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyricsdic = defaultdict(list)\n",
    "exclude = set(string.punctuation)\n",
    "lyricslist = []\n",
    "\n",
    "remove_list = ['verse','hook','chorus', '','choruslloyd banks', '50 cent', 'outro','bridge', 'chorus', 'lyrics', 'lloyd banks', 'rick ross','lil wayne', 'da', 'dem', 'im', 'ya', 'dat', 'tony yayo', 'banks yay 50',\n",
    "              'verse 1 tony yayo', 'verse 4 lloyd banks', 'verse 5 50 cent', 'verse 2 tony yayo', 'verse 1 50 cent', 'chorus 50 cent', 'intro 50 cent', 'biggie sample  in the background throughout the song', 'verse 3  tony yayo', 'verse 1  50 cent','chorus  50 cent','verse 2  lloyd banks','verse 2 lloyd banks', 'verse 6 tony yayo', 'verse 4 yony yayo']\n",
    "\n",
    "for k,v in filedic.iteritems():\n",
    "    for link in v:\n",
    "        if link != '.ipynb_checkpoints':\n",
    "            page = open('lyrics/'+k+'/'+link).read()\n",
    "            dom = DOM(page)\n",
    "            lyrics = dom('.lyrics')[0]\n",
    "            p = plaintext(lyrics.content)\n",
    "            p = ''.join(ch for ch in p if ch not in exclude)\n",
    "            lyricslist.append(p)\n",
    "            p = p.splitlines()\n",
    "            lyricsdic[k].append(filter(lambda x: x.lower() not in remove_list, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lineslist = []\n",
    "remove_list = ['verse','hook','chorus', '','choruslloyd banks', '50 cent', 'outro','bridge', 'chorus', 'lyrics', 'lloyd banks', 'rick ross','lil wayne', 'da', 'dem', 'im', 'ya', 'dat', 'tony yayo', 'banks yay 50',\n",
    "              'verse 1 tony yayo', 'verse 4 lloyd banks', 'verse 5 50 cent', 'verse 2 tony yayo', 'verse 1 50 cent', 'chorus 50 cent', 'intro 50 cent', 'biggie sample  in the background throughout the song', 'verse 3  tony yayo', 'verse 1  50 cent','chorus  50 cent','verse 2  lloyd banks','verse 2 lloyd banks', 'verse 6 tony yayo', 'verse 4 yony yayo']\n",
    "exportlist = []\n",
    "for each in lyricslist:\n",
    "    lineslist.append(each.splitlines())\n",
    "lineslist = np.array(lineslist)\n",
    "for each in lineslist:\n",
    "    exportlist.append(filter(lambda x: x.lower() not in remove_list, each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# f = open('picklelyrics', 'wb')\n",
    "\n",
    "# pickle.dump(lyricsdic, f)\n",
    "\n",
    "# # import simplejson\n",
    "# # f = open('lyricsarchive.txt', 'w')\n",
    "# # simplejson.dump(exportlist, f)\n",
    "# # f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenlist = []\n",
    "\n",
    "for lyrics in lyricslist[:100]:\n",
    "    tokens = nltk.word_tokenize(lyrics)\n",
    "    tokens = [w for w in tokens if w.lower() not in ['50','cent','chief','keef', 'lloyd', 'banks', 'busta','rhymes']]\n",
    "    tokenlist.extend([w for w in tokens if w.lower() not in stopwords.words('english')])\n",
    "\n",
    "import random\n",
    "\n",
    "class Markov(object):\n",
    "\n",
    "    def __init__(self, words):\n",
    "        self.cache = {}\n",
    "        #self.open_file = open_file\n",
    "        self.words = words\n",
    "        self.word_size = len(self.words)\n",
    "        self.database()\n",
    "\n",
    "    def file_to_words(self):\n",
    "        self.open_file.seek(0)\n",
    "        data = self.open_file.read()\n",
    "        words = data.split()\n",
    "        return words\n",
    "\n",
    "    def triples(self):\n",
    "        if len(self.words) < 3:\n",
    "            return\n",
    "        for i in range(len(self.words) - 2):\n",
    "            yield (self.words[i], self.words[i+1], self.words[i+2])\n",
    "\n",
    "    def database(self):\n",
    "        for w1, w2, w3 in self.triples():\n",
    "            key = (w1, w2)\n",
    "            if key in self.cache:\n",
    "                self.cache[key].append(w3)\n",
    "            else:\n",
    "                self.cache[key] = [w3]\n",
    "\n",
    "    def generate_markov_text(self, size=25):\n",
    "        seed = random.randint(0, self.word_size-3)\n",
    "        seed_word, next_word = self.words[seed], self.words[seed+1]\n",
    "        w1, w2 = seed_word, next_word\n",
    "        gen_words = []\n",
    "        for i in xrange(size):\n",
    "            gen_words.append(w1)\n",
    "            w1, w2 = w2, random.choice(self.cache[(w1, w2)])\n",
    "        gen_words.append(w2)\n",
    "        return ' '.join(gen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Dont start lying tire flat catching allergy attack Ill axe battle bat Saddling hos like yeeehaw Bitch enjoy tour youre fuckin Doc 5 Oclock free ride Ambulance late paper thin Hook Bun B Pass cigar Ill bust open Ill dump Dig bag tricks pull lil somethin hold medical marijuana thats break'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark = Markov(tokenlist)\n",
    "mark.generate_markov_text(size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features for each cluster:\n",
      "0: busta, rhymes, yo, yall, flipmode\n",
      "1: 50, cent, banks, lloyd, hook\n",
      "2: based, basedgod, bitches, god, lil\n",
      "3: hook, em, man, money, ya\n",
      "4: wayne, ya, money, lil, baby\n",
      "5: chief, keef, sosa, hook, bang\n",
      "6: lil, man, feel, real, love\n",
      "7: cube, ice, big, yo, ll\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df = .6)\n",
    "X = vectorizer.fit_transform(lyricslist)\n",
    "features = vectorizer.get_feature_names()\n",
    "kmeans = KMeans()\n",
    "kmeans.fit(X)\n",
    "\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-6:-1]\n",
    "print \"top features for each cluster:\"\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rhyme(inp, level):\n",
    "    entries = nltk.corpus.cmudict.entries()\n",
    "    syllables = [(word, syl) for word, syl in entries if word == inp]\n",
    "    rhymes = []\n",
    "    for (word, syllable) in syllables:\n",
    "        rhymes += [word for word, pron in entries if pron[-level:] == syllable[-level:]]\n",
    "    return set(rhymes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redman',\n",
       " 'lilb',\n",
       " 'mfdoom',\n",
       " 'lilwayne',\n",
       " 'drake',\n",
       " 'camron',\n",
       " 'llcoolj',\n",
       " 'rickross',\n",
       " 'NotoriousBIG',\n",
       " 'chiefkeef',\n",
       " 'icecube',\n",
       " 'busta',\n",
       " '50cent',\n",
       " 'lloydbanks']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
